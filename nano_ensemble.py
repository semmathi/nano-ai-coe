# -*- coding: utf-8 -*-
"""nano ensemble.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2Goc0l1MbUxRArpnOyIQ4c1uVupT3KM
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import (
    BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor,
    VotingRegressor, StackingRegressor
)

# Optional advanced boosting models
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

git remote add origin https://github.com/semmathi/nano-coe-ai.git
git branch -M main
git push -u origin main

df = pd.read_csv("nano_ai_final_aug.csv")
X = df.drop("Strength (N/mm²)", axis=1)   # Features
y = df["Strength (N/mm²)"]                # Target (continuous)
             # continuous target variable

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling (good for distance-based models like KNN)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(df.columns)

ensembles = {
    "Bagging": BaggingRegressor(estimator=DecisionTreeRegressor(),
                                n_estimators=50, random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=200, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=200, random_state=42, verbosity=0),
    "LightGBM": LGBMRegressor(n_estimators=200, random_state=42),
    "Voting": VotingRegressor(estimators=[
        ("rf", RandomForestRegressor(n_estimators=50, random_state=42)),
        ("gb", GradientBoostingRegressor(n_estimators=100, random_state=42)),
        ("xgb", XGBRegressor(n_estimators=100, random_state=42, verbosity=0))
    ]),
    "Stacking": StackingRegressor(
        estimators=[
            ("rf", RandomForestRegressor(n_estimators=50, random_state=42)),
            ("gb", GradientBoostingRegressor(n_estimators=100, random_state=42)),
            ("xgb", XGBRegressor(n_estimators=100, random_state=42, verbosity=0))
        ],
        final_estimator=RandomForestRegressor(n_estimators=50, random_state=42)
    )
}

results = []

for name, model in ensembles.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    results.append([name, r2, rmse, mae])

results_df = pd.DataFrame(results, columns=["Model", "R2 Score", "RMSE", "MAE"])
print(results_df)

# -------------------

# -------------------------------
plt.figure(figsize=(10,5))
plt.bar(results_df["Model"], results_df["R2 Score"])
plt.xticks(rotation=45)
plt.ylabel("R² Score")
plt.title("Comparison of Ensemble Regression Models")
plt.show()