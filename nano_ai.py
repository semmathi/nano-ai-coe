# -*- coding: utf-8 -*-
"""nano ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PS9fLmLXJm_q04IlWB0GOMYiCApkX7Vf
"""

!pip install pandas numpy scikit-learn matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("nano_ai_final.csv")

print("Dataset Shape:", df.shape)
df.head()



#11
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_scaled)

#10
models = {
    "Linear Regression": LinearRegression(),
    "Support Vector Regressor": SVR(kernel='rbf'),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(n_estimators=100),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100)
}

#12
from sklearn.model_selection import cross_val_score, GridSearchCV

results = {}

for name, model in models.items():

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)


    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)


    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring="r2")
    cv_mean = cv_scores.mean()


    results[name] = {
        "MAE": round(mae, 3),
        "MSE": round(mse, 3),
        "RMSE": round(rmse, 3),
        "R² (Test)": round(r2, 3),
        "CV R² (Mean)": round(cv_mean, 3)
    }

results_df = pd.DataFrame(results).T
results_df

#1
param_grid = {
    "n_estimators": [100, 200, 300],
    "max_depth": [None, 5, 10, 20],
    "min_samples_split": [2, 5, 10]
}

grid = GridSearchCV(RandomForestRegressor(random_state=42),
                    param_grid,
                    cv=5,
                    scoring="r2",
                    n_jobs=-1)

grid.fit(X_train, y_train)

print("Best Random Forest Params:", grid.best_params_)
print("Best CV Score:", grid.best_score_)


best_rf = grid.best_estimator_
y_pred_rf = best_rf.predict(X_test)
print("Tuned Random Forest R²:", r2_score(y_test, y_pred_rf))

# Rename columns properly
df.rename(columns={
    "Cement (kg/m�": "Cement (kg/m³)",
    "Fine_Agg (kg/m�": "Fine_Agg (kg/m³)",
    "Coarse_Agg (kg/m�": "Coarse_Agg (kg/m³)",
    "Strength (N/mm�": "Strength (N/mm²)"
}, inplace=True)

# Now split features and target
X = df.drop("Strength (N/mm²)", axis=1)
y = df["Strength (N/mm²)"]

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])

X = df.drop("Strength (N/mm²)", axis=1)
y = df["Strength (N/mm²)"]


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])

#error

from sklearn.model_selection import RandomizedSearchCV


param_dist = {
    "n_estimators": [100, 200, 300],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "max_depth": [3, 5, 7],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}


random_gb = RandomizedSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=20,
    cv=5,
    scoring="r2",
    n_jobs=-1,
    random_state=42
)


random_gb.fit(X_train, y_train)
p
print("Best Params (Gradient Boosting):", random_gb.best_params_)
print("Best CV R² Score:", random_gb.best_score_)


best_gb = random_gb.best_estimator_
y_pred_gb = best_gb.predict(X_test)
print("Test R² Score:", r2_score(y_test, y_pred_gb))

from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.ensemble import GradientBoostingRegressor


param_dist = {
    "n_estimators": [50, 100, 200, 300],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "max_depth": [2, 3, 4, 5],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}


random_gb = RandomizedSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=30,
    cv=5,
    scoring="r2",
    n_jobs=-1,
    random_state=42
)


random_gb.fit(X_train, y_train)


print("✅ Best Params (Gradient Boosting):", random_gb.best_params_)
print("✅ Best CV R² Score:", random_gb.best_score_)


best_gb = random_gb.best_estimator_


cv_scores = cross_val_score(best_gb, X_scaled, y, cv=5, scoring="r2")
print("Cross-Validation R² Scores:", cv_scores)
print("Mean CV R²:", cv_scores.mean())


y_pred_gb = best_gb.predict(X_test)
print("Test R² Score:", r2_score(y_test, y_pred_gb))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
import numpy as np


df["Cement_to_Silica"] = df["Cement (kg/m³)"] / (df["Nano_Silica (%)"] + 0.1)   # avoid div/0
df["Fine_to_Coarse"] = df["Fine_Agg (kg/m³)"] / (df["Coarse_Agg (kg/m³)"] + 0.1)

X = df.drop("Strength (N/mm²)", axis=1)
y = df["Strength (N/mm²)"]


X_scaled = scaler.fit_transform(X)


param_grid = {
    "n_estimators": [200, 300, 500, 800],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

grid = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring="r2",
    n_jobs=-1
)

grid.fit(X_scaled, y)

print("✅ Best Random Forest Params:", grid.best_params_)
print("✅ Best CV R² Score:", grid.best_score_)


best_rf = grid.best_estimator_
cv_scores = cross_val_score(best_rf, X_scaled, y, cv=5, scoring="r2")
print("Cross-validation R² scores:", cv_scores)
print("Mean CV R²:", cv_scores.mean())



from xgboost import XGBRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.neural_network import MLPRegressor
import matplotlib.pyplot as plt
import joblib


X = df[["Nano_Silica (%)","W/C","Plasticizer","Cement (kg/m³)","Fine_Agg (kg/m³)","Coarse_Agg (kg/m³)"]]
y = df["Strength (N/mm²)"]


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)



xgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)
print("XGBoost R²:", r2_score(y_test, y_pred_xgb))

mlp = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)
y_pred_mlp = mlp.predict(X_test)
print("Neural Network (MLP) R²:", r2_score(y_test, y_pred_mlp))



stack = StackingRegressor(
    estimators=[("rf", best_rf), ("gb", best_gb), ("xgb", xgb)],
    final_estimator=RandomForestRegressor(n_estimators=200, random_state=42)
)

stack.fit(X_train, y_train)
y_pred_stack = stack.predict(X_test)
print("Stacking Model R²:", r2_score(y_test, y_pred_stack))



cv_scores = cross_val_score(best_rf, X, y, cv=5, scoring="r2")

plt.plot(cv_scores, marker='o')
plt.title("Cross-Validation R² per Fold (Random Forest)")
plt.xlabel("Fold")
plt.ylabel("R²")
plt.show()

print("Cross-Validation R² Scores:", cv_scores)
print("Mean CV R²:", cv_scores.mean())

from xgboost import XGBRegressor
from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np


X = df[["Nano_Silica (%)","W/C","Plasticizer","Cement (kg/m³)","Fine_Agg (kg/m³)","Coarse_Agg (kg/m³)"]]
y = df["Strength (N/mm²)"]


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)



rf_params = {
    "n_estimators": [200, 300, 500],
    "max_depth": [None, 5, 10, 15],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=42),
                               param_distributions=rf_params,
                               n_iter=20, cv=5, scoring="r2", n_jobs=-1, random_state=42)
rf_search.fit(X_train, y_train)
best_rf = rf_search.best_estimator_
y_pred_rf = best_rf.predict(X_test)
print("Optimized RF R²:", r2_score(y_test, y_pred_rf))



gb_params = {
    "n_estimators": [200, 300, 500],
    "learning_rate": [0.01, 0.05, 0.1],
    "max_depth": [3, 5, 7]
}

gb_search = RandomizedSearchCV(GradientBoostingRegressor(random_state=42),
                               param_distributions=gb_params,
                               n_iter=15, cv=5, scoring="r2", n_jobs=-1, random_state=42)
gb_search.fit(X_train, y_train)
best_gb = gb_search.best_estimator_
y_pred_gb = best_gb.predict(X_test)
print("Optimized GB R²:", r2_score(y_test, y_pred_gb))



xgb = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)
print("XGBoost R²:", r2_score(y_test, y_pred_xgb))



mlp = MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=2000, alpha=0.001, random_state=42)
mlp.fit(X_train, y_train)
y_pred_mlp = mlp.predict(X_test)
print("Neural Network (MLP) R²:", r2_score(y_test, y_pred_mlp))



stack = StackingRegressor(
    estimators=[("rf", best_rf), ("gb", best_gb), ("xgb", xgb)],
    final_estimator=RandomForestRegressor(n_estimators=200, random_state=42)
)

stack.fit(X_train, y_train)
y_pred_stack = stack.predict(X_test)
print("Stacking Model R²:", r2_score(y_test, y_pred_stack))



cv_scores = cross_val_score(stack, X, y, cv=5, scoring="r2")
print("Cross-Validation R² Scores:", cv_scores)
print("Mean CV R²:", np.mean(cv_scores))

plt.plot(cv_scores, marker='o')
plt.title("Cross-Validation R² per Fold (Stacking Ensemble)")
plt.xlabel("Fold")
plt.ylabel("R²")
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor


param_grid = {
    "n_estimators": [100, 200, 300, 500],
    "max_depth": [None, 5, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

grid = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring="r2",
    n_jobs=-1
)

grid.fit(X_train, y_train)

print("Best Random Forest Params:", grid.best_params_)
print("Best CV R² Score:", grid.best_score_)

best_rf = grid.best_estimator_
y_pred_rf = best_rf.predict(X_test)

print("Tuned Random Forest Test R²:", r2_score(y_test, y_pred_rf))

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(best_rf, X_scaled, y, cv=5, scoring="r2")
print("Cross-validation R² scores:", cv_scores)
print("Mean CV R²:", cv_scores.mean())

def physics_estimate(nano, wc, plastic, cement):
    return 0.08*cement - 25*wc + 1.5*nano*(5-nano) + 2*plastic

print(physics_estimate(1.0, 0.55, 0.2, 350))  # ~20.65

# Rename columns properly
df.rename(columns={
    "Cement (kg/m�": "Cement (kg/m³)",
    "Fine_Agg (kg/m�": "Fine_Agg (kg/m³)",
    "Coarse_Agg (kg/m�": "Coarse_Agg (kg/m³)",
    "Strength (N/mm�": "Strength (N/mm²)"
}, inplace=True)

# Now split features and target
X = df.drop("Strength (N/mm²)", axis=1)
y = df["Strength (N/mm²)"]

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])

#rnn and lstm

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

#1. Data Preparation
# =========================
X = df[["Nano_Silica (%)","W/C","Plasticizer","Cement (kg/m³)","Fine_Agg (kg/m³)","Coarse_Agg (kg/m³)"]]
y = df["Strength (N/mm²)"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape to 3D for RNN/LSTM (samples, timesteps, features=1)
X_train_dl = np.expand_dims(X_train, axis=2)
X_test_dl = np.expand_dims(X_test, axis=2)

# =========================
# 2. RNN Model
# =========================
rnn_model = Sequential([
    SimpleRNN(64, activation="tanh", input_shape=(X_train_dl.shape[1], 1), return_sequences=True),
    Dropout(0.2),
    SimpleRNN(32, activation="tanh"),
    Dense(16, activation="relu"),
    Dense(1)
])

rnn_model.compile(optimizer="adam", loss="mse", metrics=["mae"])

es = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

rnn_model.fit(X_train_dl, y_train,
              validation_data=(X_test_dl, y_test),
              epochs=100, batch_size=16, callbacks=[es], verbose=1)

rnn_pred = rnn_model.predict(X_test_dl).flatten()
print("✅ RNN R²:", r2_score(y_test, rnn_pred))

# =========================
# 3. LSTM Model
# =========================
lstm_model = Sequential([
    LSTM(64, activation="tanh", input_shape=(X_train_dl.shape[1], 1), return_sequences=True),
    Dropout(0.2),
    LSTM(32, activation="tanh"),
    Dense(16, activation="relu"),
    Dense(1)
])

lstm_model.compile(optimizer="adam", loss="mse", metrics=["mae"])

lstm_model.fit(X_train_dl, y_train,
               validation_data=(X_test_dl, y_test),
               epochs=100, batch_size=16, callbacks=[es], verbose=1)

lstm_pred = lstm_model.predict(X_test_dl).flatten()
print("✅ LSTM R²:", r2_score(y_test, lstm_pred))

# 4. User Input Prediction (using LSTM as final)
# =========================
nano = float(input("Enter Nano_Silica (%): "))
wc = float(input("Enter W/C ratio: "))
plastic = float(input("Enter Plasticizer (%): "))
cement = float(input("Enter Cement (kg/m³): "))
fine = float(input("Enter Fine Aggregate (kg/m³): "))
coarse = float(input("Enter Coarse Aggregate (kg/m³): "))

user_data = pd.DataFrame([{
    "Nano_Silica (%)": nano,
    "W/C": wc,
    "Plasticizer": plastic,
    "Cement (kg/m³)": cement,
    "Fine_Agg (kg/m³)": fine,
    "Coarse_Agg (kg/m³)": coarse
}])

user_scaled = scaler.transform(user_data)
user_dl = np.expand_dims(user_scaled, axis=2)

pred_strength = lstm_model.predict(user_dl)[0][0]
print("\n✅ LSTM Predicted Concrete Strength (N/mm²):", round(pred_strength, 2))

#after argumentation.
!pip install tensorflow scikit-learn joblib

!pip install scikit-learn joblib
import tensorflow as tf
print("TF version:", tf.__version__)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import joblib

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

print("✅ Using TensorFlow:", tf.__version__)

df = pd.read_csv("/content/nano_ai_final_aug.csv")   # update path if needed

feature_cols = [
    "Nano_Silica (%)",
    "W/C",
    "Plasticizer",
    "Cement (kg/m³)",
    "Fine_Agg (kg/m³)",
    "Coarse_Agg (kg/m³)",
]
target_col = "Strength (N/mm²)"

X = df[feature_cols].copy()
y = df[target_col].copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Reshape to (samples, timesteps, features=1) for RNN/LSTM
X_train_dl = np.expand_dims(X_train, axis=2)
X_test_dl = np.expand_dims(X_test, axis=2)

print("Train shape for DL:", X_train_dl.shape)
print("Test shape for DL:", X_test_dl.shape)

# Early stopping
es = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

# ================================
# Step 4: RNN Model
# ================================
rnn_model = Sequential([
    SimpleRNN(64, activation="tanh", input_shape=(X_train_dl.shape[1], 1), return_sequences=True),
    Dropout(0.2),
    SimpleRNN(32, activation="tanh"),
    Dense(16, activation="relu"),
    Dense(1)
])
rnn_model.compile(optimizer="adam", loss="mse", metrics=["mae"])
rnn_model.fit(X_train_dl, y_train, validation_data=(X_test_dl, y_test),
              epochs=80, batch_size=32, callbacks=[es], verbose=0)

rnn_pred = rnn_model.predict(X_test_dl, verbose=0).flatten()
print("RNN R²:", r2_score(y_test, rnn_pred))

from tensorflow.keras import Input

rnn_model = Sequential([
    Input(shape=(X_train_dl.shape[1], 1)),
    SimpleRNN(64, activation="tanh", return_sequences=True),
    Dropout(0.2),
    SimpleRNN(32, activation="tanh"),
    Dense(16, activation="relu"),
    Dense(1)
])

dnn_model = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(128, activation="relu"),
    Dropout(0.3),
    Dense(64, activation="relu"),
    Dense(32, activation="relu"),
    Dense(1)
])
dnn_model.compile(optimizer="adam", loss="mse", metrics=["mae"])
dnn_model.fit(X_train, y_train, validation_data=(X_test, y_test),
              epochs=100, batch_size=16, verbose=0)

dnn_pred = dnn_model.predict(X_test).flatten()
print("DNN R²:", r2_score(y_test, dnn_pred))

#final rnn and lstm-rnn worked
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, SimpleRNN, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau


df = pd.read_csv("nano_ai_final_aug.csv")

X = df.drop("Strength (N/mm²)", axis=1)
y = df["Strength (N/mm²)"]

X["Cement_WC"] = X["Cement (kg/m³)"] / (X["W/C"] + 1e-6)
X["Fine_Coarse"] = X["Fine_Agg (kg/m³)"] / (X["Coarse_Agg (kg/m³)"] + 1e-6)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_scaled_seq = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled_seq, y, test_size=0.2, random_state=42
)

# ==========================
# Step 2: LSTM Model
# ==========================
lstm_model = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    LSTM(128, return_sequences=True),
    Dropout(0.3),
    LSTM(64),
    Dense(32, activation="relu"),
    Dense(1)
])

lstm_model.compile(optimizer="adam", loss="mse", metrics=["mae"])

# Callbacks
early_stop = EarlyStopping(monitor="val_loss", patience=20, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=10)

# Train
history = lstm_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=200,
    batch_size=16,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# Evaluate
y_pred_lstm = lstm_model.predict(X_test).flatten()
print("✅ Improved LSTM R²:", r2_score(y_test, y_pred_lstm))

# ==========================
# Step 3: RNN Model
# ==========================
rnn_model = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    SimpleRNN(128, return_sequences=True, activation="tanh"),
    Dropout(0.3),
    SimpleRNN(64, activation="tanh"),
    Dense(32, activation="relu"),
    Dense(1)
])

rnn_model.compile(optimizer="adam", loss="mse", metrics=["mae"])

rnn_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=200,
    batch_size=16,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

y_pred_rnn = rnn_model.predict(X_test).flatten()
print("✅ Improved RNN R²:", r2_score(y_test, y_pred_rnn))

# ==========================
# Step 4: Compare Models R²
# ==========================
from sklearn.metrics import r2_score

# Evaluate LSTM
y_pred_lstm = lstm_model.predict(X_test).flatten()
lstm_r2 = r2_score(y_test, y_pred_lstm)
print("✅ Improved LSTM R²:", lstm_r2)

# Evaluate RNN
y_pred_rnn = rnn_model.predict(X_test).flatten()
rnn_r2 = r2_score(y_test, y_pred_rnn)
print("✅ Improved RNN R²:", rnn_r2)

from tensorflow.keras.callbacks import EarlyStopping

es = EarlyStopping(
    monitor='val_loss',     # or 'val_r2_keras' if you prefer R²
    patience=10,            # how many epochs to wait after no improvement
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=32,
    callbacks=[es]
)

from sklearn.preprocessing import StandardScaler

X = X.reshape(len(X), -1)   # flatten (1523,6,1) → (1523,6) for scaling
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)
X_scaled = X_scaled.reshape(len(X_scaled), 6, 1)  # reshape back

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.reshape(-1,1))

import numpy as np

X = X.values   # or X.to_numpy()
X = X.reshape(len(X), -1)   # (1523,6,1) → (1523,6)

from sklearn.preprocessing import StandardScaler

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

# reshape back for RNN/LSTM
X_scaled = X_scaled.reshape(len(X_scaled), 6, 1)

y = y.values   # if y is a DataFrame/Series
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.reshape(-1,1))

from tensorflow.keras import layers, models, regularizers

inp = layers.Input(shape=(6,1))
x = layers.LSTM(128, return_sequences=False,
                dropout=0.3, recurrent_dropout=0.2,
                kernel_regularizer=regularizers.l2(1e-4))(inp)
x = layers.Dense(64, activation='relu')(x)
x = layers.Dense(32, activation='relu')(x)
out = layers.Dense(1)(x)

model = models.Model(inp, out)
model.compile(optimizer='adam', loss='mse', metrics=[r2_keras])

from sklearn.preprocessing import StandardScaler
import numpy as np

# Step 1: Convert DataFrame to NumPy (if it's still a DataFrame)
X = X.values   # Now it's a NumPy array

# Step 2: Flatten (1523,6,1) → (1523,6)
X = X.reshape(len(X), -1)

# Step 3: Scale features
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

# Step 4: Reshape back for RNN/LSTM if needed → (1523,6,1)
X_scaled = X_scaled.reshape(len(X_scaled), 6, 1)

# If X is already a NumPy array, skip .values
# Step 1: Ensure correct shape
print("Original shape of X:", X.shape)

# Step 2: Reshape (1523, 6, 1) → (1523, 6)
X = X.reshape(X.shape[0], X.shape[1])

# Step 3: Confirm new shape
print("New shape of X:", X.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(1)  # Regression output
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

from sklearn.metrics import r2_score

# Predictions
y_pred = model.predict(X_val)

# R² score
r2 = r2_score(y_val, y_pred)
print("R² on validation set:", r2)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(X)   # X shape = (1523, 6)

print(X_train.shape)

print("X_train shape before reshape:", X_train.shape)

history = model.fit(X_train, y_train,
                    validation_data=(X_test, y_test),
                    epochs=200,
                    batch_size=32,
                    callbacks=[early_stop])

import tensorflow as tf
import numpy as np
import random

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# Ensure features are scaled first
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape to 3D: (samples, timesteps=1, features)
X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_rnn  = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

print("X_train_rnn shape:", X_train_rnn.shape)  # should be (1523, 1, 6)
print("X_test_rnn shape:", X_test_rnn.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # regression output
model.compile(optimizer='adam', loss='mse')

from tensorflow.keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
history = model.fit(X_train_rnn, y_train,
                    validation_split=0.2,
                    epochs=200, batch_size=32,
                    callbacks=[es])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Build the model
model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # regression output

# Compile
model.compile(optimizer='adam', loss='mse')

# Early stopping
es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# Fit model
history = model.fit(X_train_rnn, y_train,
                    validation_data=(X_test_rnn, y_test),
                    epochs=200, batch_size=32,
                    callbacks=[es])

# Evaluate
y_pred = model.predict(X_test_rnn)
from sklearn.metrics import r2_score
print("R² Score:", r2_score(y_test, y_pred))

# -----------------------------
# 1️⃣ Set seeds for reproducibility
# -----------------------------
import tensorflow as tf
import numpy as np
import random

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# -----------------------------
# 2️⃣ Import necessary libraries
# -----------------------------
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

# -----------------------------
# 3️⃣ Assume X_train, X_test, y_train, y_test are already defined
#    For example, X_train shape: (761, 6), X_test: (191, 6)
# -----------------------------

# 3a. Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# 3b. Reshape for LSTM: (samples, timesteps, features)
X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))  # (761,1,6)
X_test_rnn  = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))    # (191,1,6)

# -----------------------------
# 4️⃣ Build the LSTM model
# -----------------------------
model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # regression output

# Compile
model.compile(optimizer='adam', loss='mse')

# Early stopping
es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# -----------------------------
# 5️⃣ Train the model
# -----------------------------
history = model.fit(X_train_rnn, y_train,
                    validation_data=(X_test_rnn, y_test),
                    epochs=200, batch_size=32,
                    callbacks=[es],
                    verbose=1)

# -----------------------------
# 6️⃣ Predict and evaluate
# -----------------------------
y_pred = model.predict(X_test_rnn)
r2 = r2_score(y_test, y_pred)
print("R² Score:", r2)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import random

# ------------------- Seed for reproducibility -------------------
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# ------------------- Assume X_train_rnn and X_test_rnn are ready -------------------
# X_train_rnn.shape = (samples, timesteps, features)
# y_train, y_test = target arrays

# ------------------- Build Pure RNN model -------------------
model_rnn = Sequential()
model_rnn.add(SimpleRNN(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model_rnn.add(Dense(32, activation='relu'))
model_rnn.add(Dense(1))  # regression output

model_rnn.compile(optimizer='adam', loss='mse', metrics=['mae'])

# ------------------- Train -------------------
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

history_rnn = model_rnn.fit(
    X_train_rnn, y_train,
    validation_data=(X_test_rnn, y_test),
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# ------------------- Evaluate -------------------
y_pred_rnn = model_rnn.predict(X_test_rnn)
r2_rnn = 1 - np.sum((y_test - y_pred_rnn.flatten())**2)/np.sum((y_test - np.mean(y_test))**2)
print("Pure RNN R² Score:", r2_rnn)

from tensorflow.keras.layers import LSTM

# ------------------- Build LSTM model -------------------
model_lstm = Sequential()
model_lstm.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model_lstm.add(Dense(32, activation='relu'))
model_lstm.add(Dense(1))

model_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])

history_lstm = model_lstm.fit(
    X_train_rnn, y_train,
    validation_data=(X_test_rnn, y_test),
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

y_pred_lstm = model_lstm.predict(X_test_rnn)
r2_lstm = 1 - np.sum((y_test - y_pred_lstm.flatten())**2)/np.sum((y_test - np.mean(y_test))**2)
print("LSTM R² Score:", r2_lstm)

# Suppose X_train.shape = (761, 6)
X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # (761, 1, 6)
X_test_rnn  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))     # (191, 1, 6)

import tensorflow as tf
import numpy as np
import random

# ---- FIX SEEDS ----
seed = 42
tf.keras.utils.set_random_seed(seed)
np.random.seed(seed)
random.seed(seed)

# ---- Keras imports ----
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# ---- Sklearn ----
from sklearn.metrics import r2_score

# Assuming X_train.shape = (num_samples, num_features)
# Reshape to 3D: (samples, timesteps, features)
X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_rnn  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

model = Sequential([
    SimpleRNN(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])),
    Dense(32, activation='relu'),
    Dense(1)  # regression output
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_rnn  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# X_train and X_test are your original datasets (2D: samples x features)
X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # 3D: (samples, timesteps, features)
X_test_rnn  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # output layer for regression

print("X_train shape before reshape:", X_train.shape)

# Reshape for RNN/LSTM
X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # (763, 1, 6)
X_test_rnn  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

print(X_train_rnn.shape)  # (763, 1, 6)
print(X_test_rnn.shape)   # (e.g., 191, 1, 6)

model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# 1. Build the model
model = Sequential()
model.add(LSTM(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # output layer for regression

# 2. Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

# 3. Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# 4. Train the model
history = model.fit(X_train_rnn, y_train,
                    validation_data=(X_test_rnn, y_test),
                    epochs=200,
                    batch_size=32,
                    callbacks=[early_stop])

import tensorflow as tf
import numpy as np
import random

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

from sklearn.metrics import r2_score
from scipy.stats import pearsonr

# 1. Predict on test data
y_pred = model.predict(X_test_rnn)

# 2. R² score
r2 = r2_score(y_test, y_pred)
print("R² Score:", r2)

# 3. Pearson correlation coefficient (r-value)
r_value, p_value = pearsonr(y_test.flatten(), y_pred.flatten())
print("Pearson r-value:", r_value)

history = model.fit(X_train, y_train,
                    validation_data=(X_test, y_test),
                    epochs=200,
                    batch_size=32,
                    callbacks=[early_stop])